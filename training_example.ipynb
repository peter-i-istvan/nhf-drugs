{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.0-cp39-cp39-win_amd64.whl (167.2 MB)\n",
      "Collecting dgl\n",
      "  Downloading dgl-0.9.1-cp39-cp39-win_amd64.whl (2.4 MB)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (2.6.3)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (1.20.3)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\anaconda\\lib\\site-packages (from dgl) (2.26.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\mate\\appdata\\roaming\\python\\python39\\site-packages (from dgl) (5.9.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in d:\\anaconda\\lib\\site-packages (from dgl) (1.7.1)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from dgl) (4.62.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\mate\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\anaconda\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mate\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->dgl) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->dgl) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->dgl) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->dgl) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\mate\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->dgl) (0.4.6)\n",
      "Installing collected packages: torch, dgl\n",
      "Successfully installed dgl-0.9.1 torch-1.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch dgl networkx pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "features_num = 10\n",
    "hidden1 = 64\n",
    "hidden2 = 32\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set torch device\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\") # training on CPU because of runtime errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model according to the code from [`dmlc/dgl`](https://github.com/dmlc/dgl)\n",
    "\n",
    "The code below will be taken from the reference implementation in the Deep Learning Graph Library (DGL). It follows the architecture shown in the source article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv # must be installed with pip\n",
    "\n",
    "class VGAEModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden1_dim, hidden2_dim):\n",
    "        super(VGAEModel, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden1_dim = hidden1_dim\n",
    "        self.hidden2_dim = hidden2_dim\n",
    "\n",
    "        layers = [\n",
    "            GraphConv(\n",
    "                self.in_dim,\n",
    "                self.hidden1_dim,\n",
    "                activation=F.relu,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "            GraphConv(\n",
    "                self.hidden1_dim,\n",
    "                self.hidden2_dim,\n",
    "                activation=lambda x: x,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "            GraphConv(\n",
    "                self.hidden1_dim,\n",
    "                self.hidden2_dim,\n",
    "                activation=lambda x: x,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "        ]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def encoder(self, g, features):\n",
    "        h = self.layers[0](g, features)\n",
    "        self.mean = self.layers[1](g, h)\n",
    "        self.log_std = self.layers[2](g, h)\n",
    "        gaussian_noise = torch.randn(features.size(0), self.hidden2_dim).to(\n",
    "            device\n",
    "        )\n",
    "        sampled_z = self.mean + gaussian_noise * torch.exp(self.log_std).to(\n",
    "            device\n",
    "        )\n",
    "        return sampled_z\n",
    "\n",
    "    def decoder(self, z):\n",
    "        adj_rec = torch.sigmoid(torch.matmul(z, z.t()))\n",
    "        return adj_rec\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        z = self.encoder(g, features)\n",
    "        adj_rec = self.decoder(z)\n",
    "        return adj_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix(\n",
    "        (adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape\n",
    "    )\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 10.0))\n",
    "    num_val = int(np.floor(edges.shape[0] / 20.0))\n",
    "\n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val : (num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(\n",
    "        edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0\n",
    "    )\n",
    "\n",
    "    def ismember(a, b, tol=5):\n",
    "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], edges_all):\n",
    "            continue\n",
    "        if test_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                continue\n",
    "        test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], val_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], val_edges):\n",
    "            continue\n",
    "        if val_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                continue\n",
    "        val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    adj_train = sp.csr_matrix(\n",
    "        (data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape\n",
    "    )\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    return (\n",
    "        adj_train,\n",
    "        train_edges,\n",
    "        val_edges,\n",
    "        val_edges_false,\n",
    "        test_edges,\n",
    "        test_edges_false,\n",
    "    )\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = (\n",
    "        adj_.dot(degree_mat_inv_sqrt)\n",
    "        .transpose()\n",
    "        .dot(degree_mat_inv_sqrt)\n",
    "        .tocoo()\n",
    "    )\n",
    "    return adj_normalized, sparse_to_tuple(adj_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our data\n",
    "\n",
    "The `load_data` function uses our own data as reference. It expects the `data/` folder to have a `features.csv` (see Milestone I.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    data = pd.read_csv(\"data/features.csv\", header = None)\n",
    "    data[0] = data[0].str.lstrip(\"D, B, 0\")\n",
    "    data[0] = data[0].astype('int')\n",
    "    adj2 = data[0].copy()\n",
    "    data = data.set_index(data.columns[0])\n",
    "    sp.csr_matrix(data.values)\n",
    "\n",
    "    graph2 = pd.read_csv(\"db/ChCh-Miner_durgbank-chem-chem.tsv.gz\", header = None)\n",
    "    graph = graph2.copy()\n",
    "    graph[0] = graph2[0].str.split('\\t').str[0]\n",
    "    graph[1] = graph2[0].str.split('\\t').str[1]\n",
    "    graph[0] = graph[0].str.lstrip(\"D, B, 0\")\n",
    "    graph[0] = graph[0].astype('int')\n",
    "    graph[1] = graph[1].str.lstrip(\"D, B, 0\")\n",
    "    graph[1] = graph[1].astype('int')\n",
    "    graph\n",
    "\n",
    "    adj = pd.DataFrame(adj2)\n",
    "    adj = adj.set_index(adj.columns[0])\n",
    "    adj[0] = np.empty((len(adj), 0)).tolist()\n",
    "\n",
    "    for g in graph.iterrows():\n",
    "        if g[1][0] in adj.index:\n",
    "            if g[1][1] in adj.index:\n",
    "                adj[0].loc[g[1][0]].insert(g[1][0],g[1][1])\n",
    "                adj[0].loc[g[1][1]].insert(g[1][1],g[1][0])\n",
    "\n",
    "    for a in adj.iterrows():\n",
    "        if a[1][0] == []:\n",
    "            adj = adj.drop(index = a[0])\n",
    "            data = data.drop(index = a[0])\n",
    "\n",
    "    features = sp.csr_matrix(data.values)/100.0\n",
    "    adj_dict = pd.Series(adj[0], index = adj.index).to_dict()\n",
    "    adjacency = nx.adjacency_matrix(nx.from_dict_of_lists(adj_dict))\n",
    "\n",
    "    return adjacency, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing steps mainly taken from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "adj, features = load_data()\n",
    "features = sparse_to_tuple(features.tocoo())\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix(\n",
    "    (\n",
    "        adj_orig.diagonal()[np.newaxis, :], [0]\n",
    "    ), shape=adj_orig.shape\n",
    ")\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "(\n",
    "    adj_train,\n",
    "    train_edges,\n",
    "    val_edges,\n",
    "    val_edges_false,\n",
    "    test_edges,\n",
    "    test_edges_false,\n",
    ") = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "# Some preprocessing\n",
    "adj_normalization, adj_norm = preprocess_graph(adj)\n",
    "\n",
    "    # Create model\n",
    "graph = dgl.from_scipy(adj_normalization)\n",
    "graph.add_self_loop()\n",
    "\n",
    "    # Create Model\n",
    "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "norm = (\n",
    "    adj.shape[0]\n",
    "    * adj.shape[0]\n",
    "    / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    ")\n",
    "\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "adj_norm = torch.sparse.FloatTensor(\n",
    "    torch.LongTensor(adj_norm[0].T),\n",
    "    torch.FloatTensor(adj_norm[1]),\n",
    "    torch.Size(adj_norm[2]),\n",
    ")\n",
    "adj_label = torch.sparse.FloatTensor(\n",
    "    torch.LongTensor(adj_label[0].T),\n",
    "    torch.FloatTensor(adj_label[1]),\n",
    "    torch.Size(adj_label[2]),\n",
    ")\n",
    "features = torch.sparse.FloatTensor(\n",
    "    torch.LongTensor(features[0].T),\n",
    "    torch.FloatTensor(features[1]),\n",
    "    torch.Size(features[2]),\n",
    ")\n",
    "\n",
    "weight_mask = adj_label.to_dense().view(-1) == 1\n",
    "weight_tensor = torch.ones(weight_mask.size(0))\n",
    "weight_tensor[weight_mask] = pos_weight\n",
    "\n",
    "features = features.to_dense()\n",
    "features_num = features.shape[-1]\n",
    "\n",
    "def get_scores(edges_pos, edges_neg, adj_rec):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        # Predict on test set of edges\n",
    "        preds = []\n",
    "        pos = []\n",
    "        for e in edges_pos:\n",
    "            # print(e)\n",
    "            # print(adj_rec[e[0], e[1]])\n",
    "            preds.append(sigmoid(adj_rec[e[0], e[1]].item()))\n",
    "            pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "        preds_neg = []\n",
    "        neg = []\n",
    "        for e in edges_neg:\n",
    "            preds_neg.append(sigmoid(adj_rec[e[0], e[1]].data))\n",
    "            neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "        preds_all = np.hstack([preds, preds_neg])\n",
    "        labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "        roc_score = roc_auc_score(labels_all, preds_all)\n",
    "        ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "        return roc_score, ap_score\n",
    "\n",
    "\n",
    "def get_acc(adj_rec, adj_label):\n",
    "    labels_all = adj_label.to_dense().view(-1).long()\n",
    "    preds_all = (adj_rec > 0.5).view(-1).long()\n",
    "    accuracy = (preds_all == labels_all).sum().float() / labels_all.size(0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgae_model = VGAEModel(features_num, hidden1, hidden2)\n",
    "# create training component\n",
    "optimizer = torch.optim.Adam(vgae_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\ttrain_loss=66886511362048.00000, train_acc=0.38762 val_roc=0.43241 val_ap=0.53634\n",
      "Epoch: 2\ttrain_loss=2116888.50000, train_acc=0.05056 val_roc=0.05919 val_ap=0.51243\n",
      "Epoch: 3\ttrain_loss=28105362.00000, train_acc=0.06686 val_roc=0.19331 val_ap=0.54370\n",
      "Epoch: 4\ttrain_loss=314594688.00000, train_acc=0.23810 val_roc=0.44239 val_ap=0.56199\n",
      "Epoch: 5\ttrain_loss=42.43733, train_acc=0.07620 val_roc=0.50820 val_ap=0.62049\n",
      "Epoch: 6\ttrain_loss=4220044.00000, train_acc=0.24123 val_roc=0.48011 val_ap=0.58237\n",
      "Epoch: 7\ttrain_loss=33.79496, train_acc=0.03963 val_roc=0.34338 val_ap=0.59494\n",
      "Epoch: 8\ttrain_loss=45.18370, train_acc=0.04424 val_roc=0.43346 val_ap=0.62298\n",
      "Epoch: 9\ttrain_loss=103.36311, train_acc=0.13120 val_roc=0.54404 val_ap=0.62602\n",
      "Epoch: 10\ttrain_loss=17.80161, train_acc=0.04336 val_roc=0.64696 val_ap=0.69182\n",
      "Epoch: 11\ttrain_loss=10.28090, train_acc=0.05816 val_roc=0.75000 val_ap=0.73493\n",
      "Epoch: 12\ttrain_loss=76837756928.00000, train_acc=0.13717 val_roc=0.66579 val_ap=0.68714\n",
      "Epoch: 13\ttrain_loss=16.28699, train_acc=0.04150 val_roc=0.67332 val_ap=0.70806\n",
      "Epoch: 14\ttrain_loss=1006358592.00000, train_acc=0.14512 val_roc=0.70596 val_ap=0.69065\n",
      "Epoch: 15\ttrain_loss=8.47902, train_acc=0.04462 val_roc=0.78323 val_ap=0.75531\n",
      "Epoch: 16\ttrain_loss=5.85244, train_acc=0.06974 val_roc=0.82421 val_ap=0.78639\n",
      "Epoch: 17\ttrain_loss=3.78946, train_acc=0.14041 val_roc=0.83492 val_ap=0.79862\n",
      "Epoch: 18\ttrain_loss=6.25414, train_acc=0.09134 val_roc=0.81069 val_ap=0.78013\n",
      "Epoch: 19\ttrain_loss=4.29998, train_acc=0.10749 val_roc=0.79967 val_ap=0.73634\n",
      "Epoch: 20\ttrain_loss=4.94679, train_acc=0.09237 val_roc=0.80853 val_ap=0.74950\n",
      "Epoch: 21\ttrain_loss=79.94664, train_acc=0.15226 val_roc=0.82996 val_ap=0.78224\n",
      "Epoch: 22\ttrain_loss=2.97230, train_acc=0.19479 val_roc=0.81578 val_ap=0.76480\n",
      "Epoch: 23\ttrain_loss=3.53858, train_acc=0.17622 val_roc=0.81670 val_ap=0.76699\n",
      "Epoch: 24\ttrain_loss=3.57361, train_acc=0.19100 val_roc=0.80307 val_ap=0.74548\n",
      "Epoch: 25\ttrain_loss=3.65737, train_acc=0.17148 val_roc=0.78875 val_ap=0.72402\n",
      "Epoch: 26\ttrain_loss=3.54626, train_acc=0.17105 val_roc=0.79484 val_ap=0.73215\n",
      "Epoch: 27\ttrain_loss=2.77165, train_acc=0.23939 val_roc=0.79757 val_ap=0.74346\n",
      "Epoch: 28\ttrain_loss=2.80126, train_acc=0.24416 val_roc=0.80655 val_ap=0.75319\n",
      "Epoch: 29\ttrain_loss=2.85216, train_acc=0.24015 val_roc=0.79074 val_ap=0.73574\n",
      "Epoch: 30\ttrain_loss=2.90575, train_acc=0.23031 val_roc=0.77496 val_ap=0.71854\n",
      "Epoch: 31\ttrain_loss=2.96150, train_acc=0.21669 val_roc=0.78180 val_ap=0.72305\n",
      "Epoch: 32\ttrain_loss=2.65931, train_acc=0.26258 val_roc=0.76986 val_ap=0.71488\n",
      "Epoch: 33\ttrain_loss=2.57226, train_acc=0.25756 val_roc=0.79423 val_ap=0.74039\n",
      "Epoch: 34\ttrain_loss=2.21667, train_acc=0.30287 val_roc=0.77576 val_ap=0.72638\n",
      "Epoch: 35\ttrain_loss=2.40392, train_acc=0.25438 val_roc=0.78437 val_ap=0.73053\n",
      "Epoch: 36\ttrain_loss=2.38327, train_acc=0.27259 val_roc=0.74710 val_ap=0.69520\n",
      "Epoch: 37\ttrain_loss=2.14761, train_acc=0.25442 val_roc=0.76835 val_ap=0.71781\n",
      "Epoch: 38\ttrain_loss=1.95966, train_acc=0.26774 val_roc=0.78862 val_ap=0.74006\n",
      "Epoch: 39\ttrain_loss=1.86968, train_acc=0.32720 val_roc=0.78144 val_ap=0.73315\n",
      "Epoch: 40\ttrain_loss=1.94516, train_acc=0.30623 val_roc=0.78902 val_ap=0.73800\n",
      "Epoch: 41\ttrain_loss=1.73502, train_acc=0.28425 val_roc=0.78340 val_ap=0.73553\n",
      "Epoch: 42\ttrain_loss=1.56863, train_acc=0.27454 val_roc=0.75830 val_ap=0.71625\n",
      "Epoch: 43\ttrain_loss=1.53699, train_acc=0.25844 val_roc=0.78556 val_ap=0.74081\n",
      "Epoch: 44\ttrain_loss=1.45277, train_acc=0.28389 val_roc=0.80464 val_ap=0.75823\n",
      "Epoch: 45\ttrain_loss=1.34312, train_acc=0.30376 val_roc=0.81921 val_ap=0.77667\n",
      "Epoch: 46\ttrain_loss=1.22695, train_acc=0.31447 val_roc=0.81084 val_ap=0.77543\n",
      "Epoch: 47\ttrain_loss=1.20037, train_acc=0.27731 val_roc=0.81065 val_ap=0.77385\n",
      "Epoch: 48\ttrain_loss=1.17594, train_acc=0.25398 val_roc=0.81398 val_ap=0.77469\n",
      "Epoch: 49\ttrain_loss=1.09634, train_acc=0.26926 val_roc=0.81893 val_ap=0.78648\n",
      "Epoch: 50\ttrain_loss=1.03413, train_acc=0.28991 val_roc=0.83385 val_ap=0.80697\n",
      "Epoch: 51\ttrain_loss=0.99473, train_acc=0.32505 val_roc=0.82965 val_ap=0.80555\n",
      "Epoch: 52\ttrain_loss=1.01157, train_acc=0.27478 val_roc=0.83243 val_ap=0.80862\n",
      "Epoch: 53\ttrain_loss=0.97071, train_acc=0.34264 val_roc=0.81009 val_ap=0.78821\n",
      "Epoch: 54\ttrain_loss=0.96009, train_acc=0.30185 val_roc=0.83270 val_ap=0.82001\n",
      "Epoch: 55\ttrain_loss=0.93794, train_acc=0.31475 val_roc=0.81814 val_ap=0.80990\n",
      "Epoch: 56\ttrain_loss=0.95170, train_acc=0.25331 val_roc=0.83520 val_ap=0.82806\n",
      "Epoch: 57\ttrain_loss=0.92948, train_acc=0.38663 val_roc=0.81528 val_ap=0.80789\n",
      "Epoch: 58\ttrain_loss=0.95202, train_acc=0.35519 val_roc=0.81186 val_ap=0.80375\n",
      "Epoch: 59\ttrain_loss=0.93401, train_acc=0.32668 val_roc=0.80705 val_ap=0.80148\n",
      "Epoch: 60\ttrain_loss=0.97534, train_acc=0.18591 val_roc=0.83953 val_ap=0.83325\n",
      "Epoch: 61\ttrain_loss=0.92201, train_acc=0.39844 val_roc=0.81762 val_ap=0.81034\n",
      "Epoch: 62\ttrain_loss=1.11249, train_acc=0.18156 val_roc=0.80270 val_ap=0.77808\n",
      "Epoch: 63\ttrain_loss=1.39514, train_acc=0.04098 val_roc=0.79021 val_ap=0.74310\n",
      "Epoch: 64\ttrain_loss=1.15666, train_acc=0.29305 val_roc=0.80058 val_ap=0.77422\n",
      "Epoch: 65\ttrain_loss=1.43326, train_acc=0.14376 val_roc=0.81089 val_ap=0.78190\n",
      "Epoch: 66\ttrain_loss=1.16354, train_acc=0.04464 val_roc=0.79279 val_ap=0.74446\n",
      "Epoch: 67\ttrain_loss=1.66297, train_acc=0.04082 val_roc=0.80020 val_ap=0.75891\n",
      "Epoch: 68\ttrain_loss=1.22981, train_acc=0.14467 val_roc=0.79840 val_ap=0.76595\n",
      "Epoch: 69\ttrain_loss=2.23497, train_acc=0.06445 val_roc=0.81350 val_ap=0.78339\n",
      "Epoch: 70\ttrain_loss=0.99619, train_acc=0.21967 val_roc=0.82251 val_ap=0.79179\n",
      "Epoch: 71\ttrain_loss=2.39497, train_acc=0.03926 val_roc=0.80132 val_ap=0.76017\n",
      "Epoch: 72\ttrain_loss=2.11923, train_acc=0.04383 val_roc=0.78576 val_ap=0.73267\n",
      "Epoch: 73\ttrain_loss=1.65545, train_acc=0.08338 val_roc=0.82653 val_ap=0.79755\n",
      "Epoch: 74\ttrain_loss=2.05980, train_acc=0.10152 val_roc=0.81820 val_ap=0.78775\n",
      "Epoch: 75\ttrain_loss=1.25112, train_acc=0.11048 val_roc=0.79599 val_ap=0.74663\n",
      "Epoch: 76\ttrain_loss=2.05203, train_acc=0.04244 val_roc=0.83432 val_ap=0.79108\n",
      "Epoch: 77\ttrain_loss=2.08414, train_acc=0.06267 val_roc=0.81742 val_ap=0.77769\n",
      "Epoch: 78\ttrain_loss=1.61433, train_acc=0.12457 val_roc=0.80552 val_ap=0.75306\n",
      "Epoch: 79\ttrain_loss=1.95976, train_acc=0.07774 val_roc=0.83938 val_ap=0.80366\n",
      "Epoch: 80\ttrain_loss=1.53694, train_acc=0.13793 val_roc=0.84752 val_ap=0.81088\n",
      "Epoch: 81\ttrain_loss=1.57495, train_acc=0.10462 val_roc=0.81139 val_ap=0.75318\n",
      "Epoch: 82\ttrain_loss=1.88072, train_acc=0.06703 val_roc=0.82924 val_ap=0.77254\n",
      "Epoch: 83\ttrain_loss=1.36573, train_acc=0.18814 val_roc=0.83203 val_ap=0.78971\n",
      "Epoch: 84\ttrain_loss=1.46095, train_acc=0.21189 val_roc=0.84032 val_ap=0.79463\n",
      "Epoch: 85\ttrain_loss=1.67605, train_acc=0.14709 val_roc=0.83811 val_ap=0.78908\n",
      "Epoch: 86\ttrain_loss=1.38911, train_acc=0.22099 val_roc=0.82442 val_ap=0.77659\n",
      "Epoch: 87\ttrain_loss=1.53174, train_acc=0.16259 val_roc=0.83374 val_ap=0.78375\n",
      "Epoch: 88\ttrain_loss=1.41278, train_acc=0.20647 val_roc=0.81498 val_ap=0.76709\n",
      "Epoch: 89\ttrain_loss=1.69557, train_acc=0.12607 val_roc=0.83018 val_ap=0.77927\n",
      "Epoch: 90\ttrain_loss=1.39053, train_acc=0.17823 val_roc=0.82780 val_ap=0.78187\n",
      "Epoch: 91\ttrain_loss=1.73050, train_acc=0.11104 val_roc=0.84186 val_ap=0.79301\n",
      "Epoch: 92\ttrain_loss=1.78946, train_acc=0.12104 val_roc=0.83616 val_ap=0.78721\n",
      "Epoch: 93\ttrain_loss=1.30284, train_acc=0.23886 val_roc=0.82793 val_ap=0.78378\n",
      "Epoch: 94\ttrain_loss=1.44514, train_acc=0.14038 val_roc=0.83924 val_ap=0.79361\n",
      "Epoch: 95\ttrain_loss=1.31001, train_acc=0.16998 val_roc=0.80718 val_ap=0.75862\n",
      "Epoch: 96\ttrain_loss=1.46101, train_acc=0.11235 val_roc=0.83929 val_ap=0.79193\n",
      "Epoch: 97\ttrain_loss=1.27546, train_acc=0.18430 val_roc=0.84309 val_ap=0.80378\n",
      "Epoch: 98\ttrain_loss=1.35766, train_acc=0.17724 val_roc=0.83825 val_ap=0.80009\n",
      "Epoch: 99\ttrain_loss=1.28677, train_acc=0.17082 val_roc=0.83202 val_ap=0.78906\n",
      "Epoch: 100\ttrain_loss=1.26234, train_acc=0.13057 val_roc=0.83634 val_ap=0.79236\n",
      "End of training!\n",
      "test_roc=0.83657\n",
      "test_ap=0.79153\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Training and validation using a full graph\n",
    "    vgae_model.train()\n",
    "    logits = vgae_model.forward(graph, features)\n",
    "\n",
    "    # compute loss\n",
    "    loss = norm * F.binary_cross_entropy(\n",
    "        logits.view(-1), adj_label.to_dense().view(-1), weight=weight_tensor\n",
    "    )\n",
    "    kl_divergence = (\n",
    "        0.5\n",
    "        / logits.size(0)\n",
    "        * (\n",
    "            1\n",
    "            + 2 * vgae_model.log_std\n",
    "            - vgae_model.mean**2\n",
    "            - torch.exp(vgae_model.log_std) ** 2\n",
    "        )\n",
    "        .sum(1)\n",
    "        .mean()\n",
    "    )\n",
    "    loss -= kl_divergence\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = get_acc(logits, adj_label)\n",
    "    val_roc, val_ap = get_scores(val_edges, val_edges_false, logits)\n",
    "    print(f'Epoch: {epoch+1}\\ttrain_loss={loss.item():.5f}, train_acc={train_acc:.5f} val_roc={val_roc:.5f} val_ap={val_ap:.5f}')\n",
    "\n",
    "test_roc, test_ap = get_scores(test_edges, test_edges_false, logits)\n",
    "print(\n",
    "    f'End of training!\\ntest_roc={test_roc:.5f}\\ntest_ap={test_ap:.5f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final area under the ROC curve (AUC): 0.836335232445804\n",
      "Final avarage precision (AP): 0.7923573781176563\n"
     ]
    }
   ],
   "source": [
    "print(\"Final area under the ROC curve (AUC):\" , val_roc)\n",
    "print(\"Final avarage precision (AP):\", val_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subject area the most common evaluations are the AUC (area under the ROC curve) and AP (avarage precision).\n",
    "The AP on the first epoch was close to 50 %, meaning it was almost random, but for the 10th epoch it became 60% (which isn`t too much, but it will be better).\n",
    "\n",
    "For more evaluation we choose the AUC metric. The reason behind it is, that the most important part of the project is to find all the interactions between drugs correctly if possible. So this metric checks that how precise is an interaction predicted by us. Meaning, if we predicted that between two drugs there is an interaction, what is the chance that there really is one. We chose this because it focuses on the finding more edges correctly, rather than finding non-edges correctly. \n",
    "As seen above this metric also went above 50% after a few epochs so it is working correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
