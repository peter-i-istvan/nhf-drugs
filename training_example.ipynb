{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.0-cp39-cp39-win_amd64.whl (167.2 MB)\n",
      "Collecting dgl\n",
      "  Downloading dgl-0.9.1-cp39-cp39-win_amd64.whl (2.4 MB)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (2.6.3)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (1.20.3)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\anaconda\\lib\\site-packages (from dgl) (2.26.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\mate\\appdata\\roaming\\python\\python39\\site-packages (from dgl) (5.9.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in d:\\anaconda\\lib\\site-packages (from dgl) (1.7.1)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from dgl) (4.62.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\mate\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\anaconda\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mate\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->dgl) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->dgl) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->dgl) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->dgl) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\mate\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->dgl) (0.4.6)\n",
      "Installing collected packages: torch, dgl\n",
      "Successfully installed dgl-0.9.1 torch-1.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch dgl networkx pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "features_num = 10\n",
    "hidden1 = 32\n",
    "hidden2 = 16\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set torch device\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\") # training on CPU because of runtime errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model according to the code from [`dmlc/dgl`](https://github.com/dmlc/dgl)\n",
    "\n",
    "The code below will be taken from the reference implementation in the Deep Learning Graph Library (DGL). It follows the architecture shown in the source article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv # must be installed with pip\n",
    "\n",
    "class VGAEModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden1_dim, hidden2_dim):\n",
    "        super(VGAEModel, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden1_dim = hidden1_dim\n",
    "        self.hidden2_dim = hidden2_dim\n",
    "\n",
    "        layers = [\n",
    "            GraphConv(\n",
    "                self.in_dim,\n",
    "                self.hidden1_dim,\n",
    "                activation=F.relu,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "            GraphConv(\n",
    "                self.hidden1_dim,\n",
    "                self.hidden2_dim,\n",
    "                activation=lambda x: x,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "            GraphConv(\n",
    "                self.hidden1_dim,\n",
    "                self.hidden2_dim,\n",
    "                activation=lambda x: x,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "        ]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def encoder(self, g, features):\n",
    "        h = self.layers[0](g, features)\n",
    "        self.mean = self.layers[1](g, h)\n",
    "        self.log_std = self.layers[2](g, h)\n",
    "        gaussian_noise = torch.randn(features.size(0), self.hidden2_dim).to(\n",
    "            device\n",
    "        )\n",
    "        sampled_z = self.mean + gaussian_noise * torch.exp(self.log_std).to(\n",
    "            device\n",
    "        )\n",
    "        return sampled_z\n",
    "\n",
    "    def decoder(self, z):\n",
    "        adj_rec = torch.sigmoid(torch.matmul(z, z.t()))\n",
    "        return adj_rec\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        z = self.encoder(g, features)\n",
    "        adj_rec = self.decoder(z)\n",
    "        return adj_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix(\n",
    "        (adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape\n",
    "    )\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 10.0))\n",
    "    num_val = int(np.floor(edges.shape[0] / 20.0))\n",
    "\n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val : (num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(\n",
    "        edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0\n",
    "    )\n",
    "\n",
    "    def ismember(a, b, tol=5):\n",
    "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], edges_all):\n",
    "            continue\n",
    "        if test_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                continue\n",
    "        test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], val_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], val_edges):\n",
    "            continue\n",
    "        if val_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                continue\n",
    "        val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    adj_train = sp.csr_matrix(\n",
    "        (data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape\n",
    "    )\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    return (\n",
    "        adj_train,\n",
    "        train_edges,\n",
    "        val_edges,\n",
    "        val_edges_false,\n",
    "        test_edges,\n",
    "        test_edges_false,\n",
    "    )\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = (\n",
    "        adj_.dot(degree_mat_inv_sqrt)\n",
    "        .transpose()\n",
    "        .dot(degree_mat_inv_sqrt)\n",
    "        .tocoo()\n",
    "    )\n",
    "    return adj_normalized, sparse_to_tuple(adj_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our data\n",
    "\n",
    "The `load_data` function uses our own data as reference. It expects the `data/` folder to have a `features.csv` (see Milestone I.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    data = pd.read_csv(\"data/features.csv\", header = None)\n",
    "    data[0] = data[0].str.lstrip(\"D, B, 0\")\n",
    "    data[0] = data[0].astype('int')\n",
    "    adj2 = data[0].copy()\n",
    "    data = data.set_index(data.columns[0])\n",
    "    sp.csr_matrix(data.values)\n",
    "\n",
    "    graph2 = pd.read_csv(\"db/ChCh-Miner_durgbank-chem-chem.tsv.gz\", header = None)\n",
    "    graph = graph2.copy()\n",
    "    graph[0] = graph2[0].str.split('\\t').str[0]\n",
    "    graph[1] = graph2[0].str.split('\\t').str[1]\n",
    "    graph[0] = graph[0].str.lstrip(\"D, B, 0\")\n",
    "    graph[0] = graph[0].astype('int')\n",
    "    graph[1] = graph[1].str.lstrip(\"D, B, 0\")\n",
    "    graph[1] = graph[1].astype('int')\n",
    "    graph\n",
    "\n",
    "    adj = pd.DataFrame(adj2)\n",
    "    adj = adj.set_index(adj.columns[0])\n",
    "    adj[0] = np.empty((len(adj), 0)).tolist()\n",
    "\n",
    "    for g in graph.iterrows():\n",
    "        if g[1][0] in adj.index:\n",
    "            if g[1][1] in adj.index:\n",
    "                adj[0].loc[g[1][0]].insert(g[1][0],g[1][1])\n",
    "                adj[0].loc[g[1][1]].insert(g[1][1],g[1][0])\n",
    "\n",
    "    for a in adj.iterrows():\n",
    "        if a[1][0] == []:\n",
    "            adj = adj.drop(index = a[0])\n",
    "            data = data.drop(index = a[0])\n",
    "\n",
    "    features = sp.csr_matrix(data.values)/100.0\n",
    "    adj_dict = pd.Series(adj[0], index = adj.index).to_dict()\n",
    "    adjacency = nx.adjacency_matrix(nx.from_dict_of_lists(adj_dict))\n",
    "\n",
    "    return adjacency, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing steps mainly taken from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "\n",
    "adj, features = load_data()\n",
    "features = sparse_to_tuple(features.tocoo())\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix(\n",
    "    (\n",
    "        adj_orig.diagonal()[np.newaxis, :], [0]\n",
    "    ), shape=adj_orig.shape\n",
    ")\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "(\n",
    "    adj_train,\n",
    "    train_edges,\n",
    "    val_edges,\n",
    "    val_edges_false,\n",
    "    test_edges,\n",
    "    test_edges_false,\n",
    ") = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "# Some preprocessing\n",
    "adj_normalization, adj_norm = preprocess_graph(adj)\n",
    "\n",
    "    # Create model\n",
    "graph = dgl.from_scipy(adj_normalization)\n",
    "graph.add_self_loop()\n",
    "\n",
    "    # Create Model\n",
    "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "norm = (\n",
    "    adj.shape[0]\n",
    "    * adj.shape[0]\n",
    "    / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    ")\n",
    "\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "adj_norm = torch.sparse.FloatTensor(\n",
    "    torch.LongTensor(adj_norm[0].T),\n",
    "    torch.FloatTensor(adj_norm[1]),\n",
    "    torch.Size(adj_norm[2]),\n",
    ")\n",
    "adj_label = torch.sparse.FloatTensor(\n",
    "    torch.LongTensor(adj_label[0].T),\n",
    "    torch.FloatTensor(adj_label[1]),\n",
    "    torch.Size(adj_label[2]),\n",
    ")\n",
    "features = torch.sparse.FloatTensor(\n",
    "    torch.LongTensor(features[0].T),\n",
    "    torch.FloatTensor(features[1]),\n",
    "    torch.Size(features[2]),\n",
    ")\n",
    "\n",
    "weight_mask = adj_label.to_dense().view(-1) == 1\n",
    "weight_tensor = torch.ones(weight_mask.size(0))\n",
    "weight_tensor[weight_mask] = pos_weight\n",
    "\n",
    "features = features.to_dense()\n",
    "features_num = features.shape[-1]\n",
    "\n",
    "def get_scores(edges_pos, edges_neg, adj_rec):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        # Predict on test set of edges\n",
    "        preds = []\n",
    "        pos = []\n",
    "        for e in edges_pos:\n",
    "            # print(e)\n",
    "            # print(adj_rec[e[0], e[1]])\n",
    "            preds.append(sigmoid(adj_rec[e[0], e[1]].item()))\n",
    "            pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "        preds_neg = []\n",
    "        neg = []\n",
    "        for e in edges_neg:\n",
    "            preds_neg.append(sigmoid(adj_rec[e[0], e[1]].data))\n",
    "            neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "        preds_all = np.hstack([preds, preds_neg])\n",
    "        labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "        roc_score = roc_auc_score(labels_all, preds_all)\n",
    "        ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "        return roc_score, ap_score\n",
    "\n",
    "\n",
    "def get_acc(adj_rec, adj_label):\n",
    "    labels_all = adj_label.to_dense().view(-1).long()\n",
    "    preds_all = (adj_rec > 0.5).view(-1).long()\n",
    "    accuracy = (preds_all == labels_all).sum().float() / labels_all.size(0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgae_model = VGAEModel(features_num, hidden1, hidden2)\n",
    "# create training component\n",
    "optimizer = torch.optim.Adam(vgae_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\ttrain_loss=13724157.00000, train_acc=0.33041 val_roc=0.36111 val_ap=0.51556\n",
      "Epoch: 2\ttrain_loss=22651.08203, train_acc=0.30621 val_roc=0.41591 val_ap=0.54428\n",
      "Epoch: 3\ttrain_loss=1374.94934, train_acc=0.29379 val_roc=0.46994 val_ap=0.57407\n",
      "Epoch: 4\ttrain_loss=163.36298, train_acc=0.30082 val_roc=0.48093 val_ap=0.57092\n",
      "Epoch: 5\ttrain_loss=41.80049, train_acc=0.31517 val_roc=0.50953 val_ap=0.57424\n",
      "Epoch: 6\ttrain_loss=19.84391, train_acc=0.33208 val_roc=0.54343 val_ap=0.58978\n",
      "Epoch: 7\ttrain_loss=12.84914, train_acc=0.35772 val_roc=0.56362 val_ap=0.60149\n",
      "Epoch: 8\ttrain_loss=9.96755, train_acc=0.38953 val_roc=0.54148 val_ap=0.58610\n",
      "Epoch: 9\ttrain_loss=8.26158, train_acc=0.41518 val_roc=0.53799 val_ap=0.58198\n",
      "Epoch: 10\ttrain_loss=6.08721, train_acc=0.41577 val_roc=0.57654 val_ap=0.60012\n",
      "End of training!\n",
      "test_roc=0.58522\n",
      "test_ap=0.61078\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Training and validation using a full graph\n",
    "    vgae_model.train()\n",
    "    logits = vgae_model.forward(graph, features)\n",
    "\n",
    "    # compute loss\n",
    "    loss = norm * F.binary_cross_entropy(\n",
    "        logits.view(-1), adj_label.to_dense().view(-1), weight=weight_tensor\n",
    "    )\n",
    "    kl_divergence = (\n",
    "        0.5\n",
    "        / logits.size(0)\n",
    "        * (\n",
    "            1\n",
    "            + 2 * vgae_model.log_std\n",
    "            - vgae_model.mean**2\n",
    "            - torch.exp(vgae_model.log_std) ** 2\n",
    "        )\n",
    "        .sum(1)\n",
    "        .mean()\n",
    "    )\n",
    "    loss -= kl_divergence\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = get_acc(logits, adj_label)\n",
    "    val_roc, val_ap = get_scores(val_edges, val_edges_false, logits)\n",
    "    print(f'Epoch: {epoch+1}\\ttrain_loss={loss.item():.5f}, train_acc={train_acc:.5f} val_roc={val_roc:.5f} val_ap={val_ap:.5f}')\n",
    "\n",
    "test_roc, test_ap = get_scores(test_edges, test_edges_false, logits)\n",
    "print(\n",
    "    f'End of training!\\ntest_roc={test_roc:.5f}\\ntest_ap={test_ap:.5f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve: 0.5765425237992534\n",
      "Avarage precision: 0.6001210885964373\n"
     ]
    }
   ],
   "source": [
    "print(\"Final area under the ROC curve (AUC):\" , val_roc)\n",
    "print(\"Final avarage precision (AP):\", val_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subject area the most common evaluations are the AUC (area under the ROC curve) and AP (avarage precision).\n",
    "The AP on the first epoch was close to 50 %, meaning it was almost random, but for the 10th epoch it became 60% (which isn`t too much, but it will be better).\n",
    "\n",
    "For more evaluation we choose the AUC metric. The reason behind it is, that the most important part of the project is to find all the interactions between drugs correctly if possible. So this metric checks that how precise is an interaction predicted by us. Meaning, if we predicted that between two drugs there is an interaction, what is the chance that there really is one. We chose this because it focuses on the finding more edges correctly, rather than finding non-edges correctly. \n",
    "As seen above this metric also went above 50% after a few epochs so it is working correctly. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
