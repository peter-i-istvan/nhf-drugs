{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch dgl networkx pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "features_num = 10\n",
    "hidden1 = 32\n",
    "hidden2 = 16\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set torch device\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\") # training on CPU because of runtime errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model according to the code from [`dmlc/dgl`](https://github.com/dmlc/dgl)\n",
    "\n",
    "The code below will be taken from the reference implementation in the Deep Learning Graph Library (DGL). It follows the architecture shown in the source article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv # must be installed with pip\n",
    "\n",
    "class VGAEModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden1_dim, hidden2_dim):\n",
    "        super(VGAEModel, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden1_dim = hidden1_dim\n",
    "        self.hidden2_dim = hidden2_dim\n",
    "\n",
    "        layers = [\n",
    "            GraphConv(\n",
    "                self.in_dim,\n",
    "                self.hidden1_dim,\n",
    "                activation=F.relu,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "            GraphConv(\n",
    "                self.hidden1_dim,\n",
    "                self.hidden2_dim,\n",
    "                activation=lambda x: x,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "            GraphConv(\n",
    "                self.hidden1_dim,\n",
    "                self.hidden2_dim,\n",
    "                activation=lambda x: x,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "        ]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def encoder(self, g, features):\n",
    "        h = self.layers[0](g, features)\n",
    "        self.mean = self.layers[1](g, h)\n",
    "        self.log_std = self.layers[2](g, h)\n",
    "        gaussian_noise = torch.randn(features.size(0), self.hidden2_dim).to(\n",
    "            device\n",
    "        )\n",
    "        sampled_z = self.mean + gaussian_noise * torch.exp(self.log_std).to(\n",
    "            device\n",
    "        )\n",
    "        return sampled_z\n",
    "\n",
    "    def decoder(self, z):\n",
    "        adj_rec = torch.sigmoid(torch.matmul(z, z.t()))\n",
    "        return adj_rec\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        z = self.encoder(g, features)\n",
    "        adj_rec = self.decoder(z)\n",
    "        return adj_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix(\n",
    "        (adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape\n",
    "    )\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 10.0))\n",
    "    num_val = int(np.floor(edges.shape[0] / 20.0))\n",
    "\n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val : (num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(\n",
    "        edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0\n",
    "    )\n",
    "\n",
    "    def ismember(a, b, tol=5):\n",
    "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], edges_all):\n",
    "            continue\n",
    "        if test_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                continue\n",
    "        test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], val_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], val_edges):\n",
    "            continue\n",
    "        if val_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                continue\n",
    "        val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    adj_train = sp.csr_matrix(\n",
    "        (data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape\n",
    "    )\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    return (\n",
    "        adj_train,\n",
    "        train_edges,\n",
    "        val_edges,\n",
    "        val_edges_false,\n",
    "        test_edges,\n",
    "        test_edges_false,\n",
    "    )\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = (\n",
    "        adj_.dot(degree_mat_inv_sqrt)\n",
    "        .transpose()\n",
    "        .dot(degree_mat_inv_sqrt)\n",
    "        .tocoo()\n",
    "    )\n",
    "    return adj_normalized, sparse_to_tuple(adj_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our data\n",
    "\n",
    "The `load_data` function uses our own data as reference. It expects the `data/` folder to have a `features.csv` (see Milestone I.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    data = pd.read_csv(\"data/features.csv\", header = None)\n",
    "    data[0] = data[0].str.lstrip(\"D, B, 0\")\n",
    "    data[0] = data[0].astype('int')\n",
    "    adj2 = data[0].copy()\n",
    "    data = data.set_index(data.columns[0])\n",
    "    sp.csr_matrix(data.values)\n",
    "\n",
    "    graph2 = pd.read_csv(\"db/ChCh-Miner_durgbank-chem-chem.tsv.gz\", header = None)\n",
    "    graph = graph2.copy()\n",
    "    graph[0] = graph2[0].str.split('\\t').str[0]\n",
    "    graph[1] = graph2[0].str.split('\\t').str[1]\n",
    "    graph[0] = graph[0].str.lstrip(\"D, B, 0\")\n",
    "    graph[0] = graph[0].astype('int')\n",
    "    graph[1] = graph[1].str.lstrip(\"D, B, 0\")\n",
    "    graph[1] = graph[1].astype('int')\n",
    "    graph\n",
    "\n",
    "    adj = pd.DataFrame(adj2)\n",
    "    adj = adj.set_index(adj.columns[0])\n",
    "    adj[0] = np.empty((len(adj), 0)).tolist()\n",
    "\n",
    "    for g in graph.iterrows():\n",
    "        if g[1][0] in adj.index:\n",
    "            if g[1][1] in adj.index:\n",
    "                adj[0].loc[g[1][0]].insert(g[1][0],g[1][1])\n",
    "                adj[0].loc[g[1][1]].insert(g[1][1],g[1][0])\n",
    "\n",
    "    for a in adj.iterrows():\n",
    "        if a[1][0] == []:\n",
    "            adj = adj.drop(index = a[0])\n",
    "            data = data.drop(index = a[0])\n",
    "\n",
    "    features = sp.csr_matrix(data.values)/100.0\n",
    "    adj_dict = pd.Series(adj[0], index = adj.index).to_dict()\n",
    "    adjacency = nx.adjacency_matrix(nx.from_dict_of_lists(adj_dict))\n",
    "\n",
    "    return adjacency, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing steps mainly taken from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_849015/964416747.py:44: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adjacency = nx.adjacency_matrix(nx.from_dict_of_lists(adj_dict))\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "\n",
    "adj, features = load_data()\n",
    "features = sparse_to_tuple(features.tocoo())\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix(\n",
    "    (\n",
    "        adj_orig.diagonal()[np.newaxis, :], [0]\n",
    "    ), shape=adj_orig.shape\n",
    ")\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "(\n",
    "    adj_train,\n",
    "    train_edges,\n",
    "    val_edges,\n",
    "    val_edges_false,\n",
    "    test_edges,\n",
    "    test_edges_false,\n",
    ") = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "# Some preprocessing\n",
    "adj_normalization, adj_norm = preprocess_graph(adj)\n",
    "\n",
    "    # Create model\n",
    "graph = dgl.from_scipy(adj_normalization)\n",
    "graph.add_self_loop()\n",
    "\n",
    "    # Create Model\n",
    "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "norm = (\n",
    "    adj.shape[0]\n",
    "    * adj.shape[0]\n",
    "    / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    ")\n",
    "\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "adj_norm = torch.sparse.FloatTensor(\n",
    "    torch.LongTensor(adj_norm[0].T),\n",
    "    torch.FloatTensor(adj_norm[1]),\n",
    "    torch.Size(adj_norm[2]),\n",
    ")\n",
    "adj_label = torch.sparse.FloatTensor(\n",
    "    torch.LongTensor(adj_label[0].T),\n",
    "    torch.FloatTensor(adj_label[1]),\n",
    "    torch.Size(adj_label[2]),\n",
    ")\n",
    "features = torch.sparse.FloatTensor(\n",
    "    torch.LongTensor(features[0].T),\n",
    "    torch.FloatTensor(features[1]),\n",
    "    torch.Size(features[2]),\n",
    ")\n",
    "\n",
    "weight_mask = adj_label.to_dense().view(-1) == 1\n",
    "weight_tensor = torch.ones(weight_mask.size(0))\n",
    "weight_tensor[weight_mask] = pos_weight\n",
    "\n",
    "features = features.to_dense()\n",
    "features_num = features.shape[-1]\n",
    "\n",
    "def get_scores(edges_pos, edges_neg, adj_rec):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        # Predict on test set of edges\n",
    "        preds = []\n",
    "        pos = []\n",
    "        for e in edges_pos:\n",
    "            # print(e)\n",
    "            # print(adj_rec[e[0], e[1]])\n",
    "            preds.append(sigmoid(adj_rec[e[0], e[1]].item()))\n",
    "            pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "        preds_neg = []\n",
    "        neg = []\n",
    "        for e in edges_neg:\n",
    "            preds_neg.append(sigmoid(adj_rec[e[0], e[1]].data))\n",
    "            neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "        preds_all = np.hstack([preds, preds_neg])\n",
    "        labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "        roc_score = roc_auc_score(labels_all, preds_all)\n",
    "        ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "        return roc_score, ap_score\n",
    "\n",
    "\n",
    "def get_acc(adj_rec, adj_label):\n",
    "    labels_all = adj_label.to_dense().view(-1).long()\n",
    "    preds_all = (adj_rec > 0.5).view(-1).long()\n",
    "    accuracy = (preds_all == labels_all).sum().float() / labels_all.size(0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgae_model = VGAEModel(features_num, hidden1, hidden2)\n",
    "# create training component\n",
    "optimizer = torch.optim.Adam(vgae_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\ttrain_loss=2256143.25000, train_acc=0.36272 val_roc=0.52644 val_ap=0.58502\n",
      "Epoch: 2\ttrain_loss=172.43683, train_acc=0.33002 val_roc=0.58926 val_ap=0.61743\n",
      "Epoch: 3\ttrain_loss=6.38285, train_acc=0.32701 val_roc=0.61644 val_ap=0.62239\n",
      "Epoch: 4\ttrain_loss=3.55159, train_acc=0.34413 val_roc=0.64832 val_ap=0.64209\n",
      "Epoch: 5\ttrain_loss=2.36837, train_acc=0.36836 val_roc=0.65132 val_ap=0.63630\n",
      "Epoch: 6\ttrain_loss=1.97768, train_acc=0.40257 val_roc=0.65641 val_ap=0.63406\n",
      "Epoch: 7\ttrain_loss=1.76602, train_acc=0.42281 val_roc=0.63417 val_ap=0.61000\n",
      "Epoch: 8\ttrain_loss=1.64636, train_acc=0.43631 val_roc=0.61507 val_ap=0.58191\n",
      "Epoch: 9\ttrain_loss=1.59470, train_acc=0.44618 val_roc=0.60673 val_ap=0.57952\n",
      "Epoch: 10\ttrain_loss=1.47292, train_acc=0.45012 val_roc=0.62056 val_ap=0.58413\n",
      "End of training!\n",
      "test_roc=0.60987\n",
      "test_ap=0.58271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Training and validation using a full graph\n",
    "    vgae_model.train()\n",
    "    logits = vgae_model.forward(graph, features)\n",
    "\n",
    "    # compute loss\n",
    "    loss = norm * F.binary_cross_entropy(\n",
    "        logits.view(-1), adj_label.to_dense().view(-1), weight=weight_tensor\n",
    "    )\n",
    "    kl_divergence = (\n",
    "        0.5\n",
    "        / logits.size(0)\n",
    "        * (\n",
    "            1\n",
    "            + 2 * vgae_model.log_std\n",
    "            - vgae_model.mean**2\n",
    "            - torch.exp(vgae_model.log_std) ** 2\n",
    "        )\n",
    "        .sum(1)\n",
    "        .mean()\n",
    "    )\n",
    "    loss -= kl_divergence\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = get_acc(logits, adj_label)\n",
    "    val_roc, val_ap = get_scores(val_edges, val_edges_false, logits)\n",
    "    print(f'Epoch: {epoch+1}\\ttrain_loss={loss.item():.5f}, train_acc={train_acc:.5f} val_roc={val_roc:.5f} val_ap={val_ap:.5f}')\n",
    "\n",
    "test_roc, test_ap = get_scores(test_edges, test_edges_false, logits)\n",
    "print(\n",
    "    f'End of training!\\ntest_roc={test_roc:.5f}\\ntest_ap={test_ap:.5f}'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e24387181c4ade438ada53bbca295e7b1d979ac5797f7fff3cf25ab543e13105"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
