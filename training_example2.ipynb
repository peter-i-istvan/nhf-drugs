{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.0-cp310-cp310-win_amd64.whl (167.3 MB)\n",
      "     -------------------------------------- 167.3/167.3 MB 3.0 MB/s eta 0:00:00\n",
      "Collecting dgl\n",
      "  Downloading dgl-0.9.1-cp310-cp310-win_amd64.whl (2.4 MB)\n",
      "     ---------------------------------------- 2.4/2.4 MB 6.4 MB/s eta 0:00:00\n",
      "Collecting networkx\n",
      "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 6.1 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.5.2-cp310-cp310-win_amd64.whl (10.4 MB)\n",
      "     ---------------------------------------- 10.4/10.4 MB 4.1 MB/s eta 0:00:00\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.23.5-cp310-cp310-win_amd64.whl (14.6 MB)\n",
      "     ---------------------------------------- 14.6/14.6 MB 5.9 MB/s eta 0:00:00\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting requests>=2.19.0\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.8/62.8 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading scipy-1.9.3-cp310-cp310-win_amd64.whl (40.1 MB)\n",
      "     -------------------------------------- 40.1/40.1 MB 502.0 kB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\bence\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from dgl) (5.9.4)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.6-py2.py3-none-any.whl (498 kB)\n",
      "     -------------------------------------- 498.1/498.1 kB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\bence\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bence\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "     ------------------------------------ 140.6/140.6 kB 225.5 kB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.5/61.5 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "     -------------------------------------- 155.3/155.3 kB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\bence\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm->dgl) (0.4.6)\n",
      "Installing collected packages: pytz, urllib3, typing-extensions, tqdm, numpy, networkx, idna, charset-normalizer, certifi, torch, scipy, requests, pandas, dgl\n",
      "Successfully installed certifi-2022.12.7 charset-normalizer-2.1.1 dgl-0.9.1 idna-3.4 networkx-2.8.8 numpy-1.23.5 pandas-1.5.2 pytz-2022.6 requests-2.28.1 scipy-1.9.3 torch-1.13.0 tqdm-4.64.1 typing-extensions-4.4.0 urllib3-1.26.13\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\bence\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\bence\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script normalizer.exe is installed in 'C:\\Users\\bence\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\bence\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch dgl networkx pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.0-cp310-cp310-win_amd64.whl (8.2 MB)\n",
      "     ---------------------------------------- 8.2/8.2 MB 415.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\bence\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\bence\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn) (1.9.3)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ------------------------------------ 298.0/298.0 kB 708.7 kB/s eta 0:00:00\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.0 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "features_num = 10\n",
    "hidden1 = 64\n",
    "hidden2 = 32\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set torch device\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\") # training on CPU because of runtime errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model according to the code from [`dmlc/dgl`](https://github.com/dmlc/dgl)\n",
    "\n",
    "The code below will be taken from the reference implementation in the Deep Learning Graph Library (DGL). It follows the architecture shown in the source article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv # must be installed with pip\n",
    "\n",
    "class VGAEModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden1_dim, hidden2_dim):\n",
    "        super(VGAEModel, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden1_dim = hidden1_dim\n",
    "        self.hidden2_dim = hidden2_dim\n",
    "\n",
    "        layers = [\n",
    "            GraphConv(\n",
    "                self.in_dim,\n",
    "                self.hidden1_dim,\n",
    "                activation=F.relu,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "            GraphConv(\n",
    "                self.hidden1_dim,\n",
    "                self.hidden2_dim,\n",
    "                activation=lambda x: x,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "            GraphConv(\n",
    "                self.hidden1_dim,\n",
    "                self.hidden2_dim,\n",
    "                activation=lambda x: x,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "            GraphConv(\n",
    "                32,\n",
    "                16,\n",
    "                activation=lambda x: x,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "        ]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def encoder(self, g, features):\n",
    "        h = self.layers[0](g, features)\n",
    "        self.mean = self.layers[1](g, h)\n",
    "        self.log_std = self.layers[2](g, h)\n",
    "        gaussian_noise = torch.randn(features.size(0), self.hidden2_dim).to(\n",
    "            device\n",
    "        )\n",
    "        sampled_z = self.mean + gaussian_noise * torch.exp(self.log_std).to(\n",
    "            device\n",
    "        )\n",
    "        return sampled_z\n",
    "\n",
    "    def decoder(self, z):\n",
    "        adj_rec = torch.sigmoid(torch.matmul(z, z.t()))\n",
    "        return adj_rec\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        z = self.encoder(g, features)\n",
    "        adj_rec = self.decoder(z)\n",
    "        return adj_rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix(\n",
    "        (adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape\n",
    "    )\n",
    "    adj.eliminate_zeros()\n",
    "    # Check that diag is zero:\n",
    "    assert np.diag(adj.todense()).sum() == 0\n",
    "\n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 10.0))\n",
    "    num_val = int(np.floor(edges.shape[0] / 20.0))\n",
    "\n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val : (num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(\n",
    "        edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0\n",
    "    )\n",
    "\n",
    "    def ismember(a, b, tol=5):\n",
    "        rows_close = np.all(np.round(a - b[:, None], tol) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], edges_all):\n",
    "            continue\n",
    "        if test_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                continue\n",
    "        test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        idx_i = np.random.randint(0, adj.shape[0])\n",
    "        idx_j = np.random.randint(0, adj.shape[0])\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], train_edges):\n",
    "            continue\n",
    "        if ismember([idx_i, idx_j], val_edges):\n",
    "            continue\n",
    "        if ismember([idx_j, idx_i], val_edges):\n",
    "            continue\n",
    "        if val_edges_false:\n",
    "            if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                continue\n",
    "        val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    adj_train = sp.csr_matrix(\n",
    "        (data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape\n",
    "    )\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    return (\n",
    "        adj_train,\n",
    "        train_edges,\n",
    "        val_edges,\n",
    "        val_edges_false,\n",
    "        test_edges,\n",
    "        test_edges_false,\n",
    "    )\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = (\n",
    "        adj_.dot(degree_mat_inv_sqrt)\n",
    "        .transpose()\n",
    "        .dot(degree_mat_inv_sqrt)\n",
    "        .tocoo()\n",
    "    )\n",
    "    return adj_normalized, sparse_to_tuple(adj_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our data\n",
    "\n",
    "The `load_data` function uses our own data as reference. It expects the `data/` folder to have a `features.csv` (see Milestone I.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    data = pd.read_csv(\"data/features.csv\", header = None)\n",
    "    data[0] = data[0].str.lstrip(\"D, B, 0\")\n",
    "    data[0] = data[0].astype('int')\n",
    "    adj2 = data[0].copy()\n",
    "    data = data.set_index(data.columns[0])\n",
    "    sp.csr_matrix(data.values)\n",
    "\n",
    "    graph2 = pd.read_csv(\"db/ChCh-Miner_durgbank-chem-chem.tsv.gz\", header = None)\n",
    "    graph = graph2.copy()\n",
    "    graph[0] = graph2[0].str.split('\\t').str[0]\n",
    "    graph[1] = graph2[0].str.split('\\t').str[1]\n",
    "    graph[0] = graph[0].str.lstrip(\"D, B, 0\")\n",
    "    graph[0] = graph[0].astype('int')\n",
    "    graph[1] = graph[1].str.lstrip(\"D, B, 0\")\n",
    "    graph[1] = graph[1].astype('int')\n",
    "    graph\n",
    "\n",
    "    adj = pd.DataFrame(adj2)\n",
    "    adj = adj.set_index(adj.columns[0])\n",
    "    adj[0] = np.empty((len(adj), 0)).tolist()\n",
    "\n",
    "    for g in graph.iterrows():\n",
    "        if g[1][0] in adj.index:\n",
    "            if g[1][1] in adj.index:\n",
    "                adj[0].loc[g[1][0]].insert(g[1][0],g[1][1])\n",
    "                adj[0].loc[g[1][1]].insert(g[1][1],g[1][0])\n",
    "\n",
    "    for a in adj.iterrows():\n",
    "        if a[1][0] == []:\n",
    "            adj = adj.drop(index = a[0])\n",
    "            data = data.drop(index = a[0])\n",
    "\n",
    "    features = sp.csr_matrix(data.values)/100.0\n",
    "    adj_dict = pd.Series(adj[0], index = adj.index).to_dict()\n",
    "    adjacency = nx.adjacency_matrix(nx.from_dict_of_lists(adj_dict))\n",
    "\n",
    "    return adjacency, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing steps mainly taken from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bence\\AppData\\Local\\Temp\\ipykernel_10364\\964416747.py:44: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adjacency = nx.adjacency_matrix(nx.from_dict_of_lists(adj_dict))\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "\n",
    "adj, features = load_data()\n",
    "features = sparse_to_tuple(features.tocoo())\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj\n",
    "adj_orig = adj_orig - sp.dia_matrix(\n",
    "    (\n",
    "        adj_orig.diagonal()[np.newaxis, :], [0]\n",
    "    ), shape=adj_orig.shape\n",
    ")\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "(\n",
    "    adj_train,\n",
    "    train_edges,\n",
    "    val_edges,\n",
    "    val_edges_false,\n",
    "    test_edges,\n",
    "    test_edges_false,\n",
    ") = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "# Some preprocessing\n",
    "adj_normalization, adj_norm = preprocess_graph(adj)\n",
    "\n",
    "    # Create model\n",
    "graph = dgl.from_scipy(adj_normalization)\n",
    "graph.add_self_loop()\n",
    "\n",
    "    # Create Model\n",
    "pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n",
    "norm = (\n",
    "    adj.shape[0]\n",
    "    * adj.shape[0]\n",
    "    / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    ")\n",
    "\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "\n",
    "adj_norm = torch.sparse.FloatTensor(\n",
    "    torch.LongTensor(adj_norm[0].T),\n",
    "    torch.FloatTensor(adj_norm[1]),\n",
    "    torch.Size(adj_norm[2]),\n",
    ")\n",
    "adj_label = torch.sparse.FloatTensor(\n",
    "    torch.LongTensor(adj_label[0].T),\n",
    "    torch.FloatTensor(adj_label[1]),\n",
    "    torch.Size(adj_label[2]),\n",
    ")\n",
    "features = torch.sparse.FloatTensor(\n",
    "    torch.LongTensor(features[0].T),\n",
    "    torch.FloatTensor(features[1]),\n",
    "    torch.Size(features[2]),\n",
    ")\n",
    "\n",
    "weight_mask = adj_label.to_dense().view(-1) == 1\n",
    "weight_tensor = torch.ones(weight_mask.size(0))\n",
    "weight_tensor[weight_mask] = pos_weight\n",
    "\n",
    "features = features.to_dense()\n",
    "features_num = features.shape[-1]\n",
    "\n",
    "def get_scores(edges_pos, edges_neg, adj_rec):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        # Predict on test set of edges\n",
    "        preds = []\n",
    "        pos = []\n",
    "        for e in edges_pos:\n",
    "            # print(e)\n",
    "            # print(adj_rec[e[0], e[1]])\n",
    "            preds.append(sigmoid(adj_rec[e[0], e[1]].item()))\n",
    "            pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "        preds_neg = []\n",
    "        neg = []\n",
    "        for e in edges_neg:\n",
    "            preds_neg.append(sigmoid(adj_rec[e[0], e[1]].data))\n",
    "            neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "        preds_all = np.hstack([preds, preds_neg])\n",
    "        labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds_neg))])\n",
    "        roc_score = roc_auc_score(labels_all, preds_all)\n",
    "        ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "        return roc_score, ap_score\n",
    "\n",
    "\n",
    "def get_acc(adj_rec, adj_label):\n",
    "    labels_all = adj_label.to_dense().view(-1).long()\n",
    "    preds_all = (adj_rec > 0.5).view(-1).long()\n",
    "    accuracy = (preds_all == labels_all).sum().float() / labels_all.size(0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgae_model = VGAEModel(features_num, hidden1, hidden2)\n",
    "# create training component\n",
    "optimizer = torch.optim.Adam(vgae_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\ttrain_loss=1199492255575965763108864.00000, train_acc=0.31998 val_roc=0.53340 val_ap=0.58838\n",
      "Epoch: 2\ttrain_loss=297224838663756578816.00000, train_acc=0.06918 val_roc=0.41602 val_ap=0.61604\n",
      "Epoch: 3\ttrain_loss=7006092490235183104.00000, train_acc=0.15584 val_roc=0.67828 val_ap=0.69654\n",
      "Epoch: 4\ttrain_loss=11653567479808.00000, train_acc=0.17141 val_roc=0.71765 val_ap=0.72049\n",
      "Epoch: 5\ttrain_loss=31227.90625, train_acc=0.11562 val_roc=0.79562 val_ap=0.78409\n",
      "Epoch: 6\ttrain_loss=6.13615, train_acc=0.13577 val_roc=0.82421 val_ap=0.79757\n",
      "Epoch: 7\ttrain_loss=186953.62500, train_acc=0.14591 val_roc=0.83062 val_ap=0.79561\n",
      "Epoch: 8\ttrain_loss=5.87275, train_acc=0.13981 val_roc=0.83317 val_ap=0.80820\n",
      "Epoch: 9\ttrain_loss=4.74576, train_acc=0.14507 val_roc=0.83206 val_ap=0.79558\n",
      "Epoch: 10\ttrain_loss=4.26832, train_acc=0.13891 val_roc=0.82996 val_ap=0.77946\n",
      "Epoch: 11\ttrain_loss=3.79865, train_acc=0.17372 val_roc=0.82438 val_ap=0.77920\n",
      "Epoch: 12\ttrain_loss=3.94152, train_acc=0.20599 val_roc=0.77433 val_ap=0.73263\n",
      "Epoch: 13\ttrain_loss=4.11145, train_acc=0.12761 val_roc=0.82601 val_ap=0.77338\n",
      "Epoch: 14\ttrain_loss=1030594.75000, train_acc=0.17780 val_roc=0.82295 val_ap=0.76708\n",
      "Epoch: 15\ttrain_loss=3.12244, train_acc=0.15597 val_roc=0.82733 val_ap=0.77026\n",
      "Epoch: 16\ttrain_loss=40.89014, train_acc=0.16478 val_roc=0.82581 val_ap=0.77555\n",
      "Epoch: 17\ttrain_loss=2.65668, train_acc=0.20091 val_roc=0.81296 val_ap=0.76520\n",
      "Epoch: 18\ttrain_loss=2.73261, train_acc=0.16807 val_roc=0.80905 val_ap=0.74474\n",
      "Epoch: 19\ttrain_loss=2.57549, train_acc=0.18122 val_roc=0.81430 val_ap=0.76704\n",
      "Epoch: 20\ttrain_loss=2.72355, train_acc=0.18537 val_roc=0.77560 val_ap=0.73400\n",
      "Epoch: 21\ttrain_loss=2.07305, train_acc=0.21233 val_roc=0.80876 val_ap=0.75564\n",
      "Epoch: 22\ttrain_loss=2.31315, train_acc=0.16252 val_roc=0.83012 val_ap=0.77591\n",
      "Epoch: 23\ttrain_loss=1.97246, train_acc=0.20804 val_roc=0.81662 val_ap=0.77371\n",
      "Epoch: 24\ttrain_loss=1.98747, train_acc=0.16748 val_roc=0.82374 val_ap=0.76776\n",
      "Epoch: 25\ttrain_loss=75.25363, train_acc=0.19059 val_roc=0.81141 val_ap=0.76542\n",
      "Epoch: 26\ttrain_loss=1.81366, train_acc=0.15573 val_roc=0.82368 val_ap=0.77260\n",
      "Epoch: 27\ttrain_loss=1.59703, train_acc=0.18740 val_roc=0.81590 val_ap=0.76627\n",
      "Epoch: 28\ttrain_loss=1.57053, train_acc=0.18124 val_roc=0.83248 val_ap=0.77955\n",
      "Epoch: 29\ttrain_loss=1.37684, train_acc=0.21175 val_roc=0.82562 val_ap=0.78663\n",
      "Epoch: 30\ttrain_loss=1.39871, train_acc=0.17968 val_roc=0.83393 val_ap=0.79026\n",
      "Epoch: 31\ttrain_loss=1.21917, train_acc=0.19255 val_roc=0.82163 val_ap=0.78255\n",
      "Epoch: 32\ttrain_loss=1.17619, train_acc=0.17772 val_roc=0.83063 val_ap=0.79019\n",
      "Epoch: 33\ttrain_loss=1.12096, train_acc=0.16789 val_roc=0.83388 val_ap=0.79096\n",
      "Epoch: 34\ttrain_loss=0.94767, train_acc=0.22092 val_roc=0.82224 val_ap=0.79231\n",
      "Epoch: 35\ttrain_loss=0.97777, train_acc=0.15765 val_roc=0.85484 val_ap=0.82100\n",
      "Epoch: 36\ttrain_loss=1.44334, train_acc=0.20510 val_roc=0.82871 val_ap=0.79717\n",
      "Epoch: 37\ttrain_loss=0.90581, train_acc=0.14994 val_roc=0.83844 val_ap=0.80334\n",
      "Epoch: 38\ttrain_loss=0.87450, train_acc=0.19436 val_roc=0.82566 val_ap=0.80390\n",
      "Epoch: 39\ttrain_loss=0.89250, train_acc=0.19712 val_roc=0.81578 val_ap=0.78701\n",
      "Epoch: 40\ttrain_loss=0.96399, train_acc=0.14931 val_roc=0.79983 val_ap=0.75574\n",
      "Epoch: 41\ttrain_loss=0.97699, train_acc=0.18130 val_roc=0.80562 val_ap=0.80082\n",
      "Epoch: 42\ttrain_loss=0.91308, train_acc=0.20666 val_roc=0.80092 val_ap=0.77736\n",
      "Epoch: 43\ttrain_loss=1.33690, train_acc=0.11066 val_roc=0.78018 val_ap=0.71456\n",
      "Epoch: 44\ttrain_loss=0.90866, train_acc=0.23414 val_roc=0.80473 val_ap=0.79579\n",
      "Epoch: 45\ttrain_loss=1.12080, train_acc=0.14608 val_roc=0.82274 val_ap=0.80897\n",
      "Epoch: 46\ttrain_loss=1.12238, train_acc=0.28172 val_roc=0.72800 val_ap=0.66506\n",
      "Epoch: 47\ttrain_loss=6.00110, train_acc=0.09443 val_roc=0.81330 val_ap=0.77182\n",
      "Epoch: 48\ttrain_loss=0.91020, train_acc=0.20672 val_roc=0.78836 val_ap=0.77994\n",
      "Epoch: 49\ttrain_loss=1.88124, train_acc=0.05398 val_roc=0.79750 val_ap=0.75315\n",
      "Epoch: 50\ttrain_loss=1.01980, train_acc=0.16204 val_roc=0.85154 val_ap=0.82628\n",
      "Epoch: 51\ttrain_loss=2.60283, train_acc=0.04552 val_roc=0.83951 val_ap=0.80570\n",
      "Epoch: 52\ttrain_loss=1.17485, train_acc=0.12493 val_roc=0.83763 val_ap=0.80967\n",
      "Epoch: 53\ttrain_loss=3.36421, train_acc=0.04282 val_roc=0.79528 val_ap=0.74666\n",
      "Epoch: 54\ttrain_loss=1.22625, train_acc=0.13364 val_roc=0.83735 val_ap=0.79344\n",
      "Epoch: 55\ttrain_loss=3.04205, train_acc=0.05861 val_roc=0.84812 val_ap=0.82021\n",
      "Epoch: 56\ttrain_loss=1.48226, train_acc=0.16945 val_roc=0.82762 val_ap=0.80816\n",
      "Epoch: 57\ttrain_loss=3.07347, train_acc=0.05411 val_roc=0.80211 val_ap=0.74895\n",
      "Epoch: 58\ttrain_loss=1.69558, train_acc=0.12854 val_roc=0.82580 val_ap=0.78690\n",
      "Epoch: 59\ttrain_loss=1.98700, train_acc=0.12378 val_roc=0.84241 val_ap=0.80634\n",
      "Epoch: 60\ttrain_loss=1.73524, train_acc=0.12623 val_roc=0.83839 val_ap=0.78564\n",
      "Epoch: 61\ttrain_loss=1.87608, train_acc=0.14201 val_roc=0.84059 val_ap=0.80476\n",
      "Epoch: 62\ttrain_loss=1.48055, train_acc=0.19450 val_roc=0.81722 val_ap=0.78218\n",
      "Epoch: 63\ttrain_loss=2.14292, train_acc=0.08275 val_roc=0.80683 val_ap=0.74917\n",
      "Epoch: 64\ttrain_loss=1.53522, train_acc=0.19409 val_roc=0.81071 val_ap=0.78413\n",
      "Epoch: 65\ttrain_loss=1.60125, train_acc=0.16004 val_roc=0.83448 val_ap=0.79302\n",
      "Epoch: 66\ttrain_loss=2.35549, train_acc=0.07135 val_roc=0.79167 val_ap=0.71289\n",
      "Epoch: 67\ttrain_loss=1.39774, train_acc=0.20650 val_roc=0.81663 val_ap=0.79155\n",
      "Epoch: 68\ttrain_loss=1.59607, train_acc=0.18640 val_roc=0.82363 val_ap=0.79935\n",
      "Epoch: 69\ttrain_loss=1.52244, train_acc=0.16626 val_roc=0.79574 val_ap=0.74003\n",
      "Epoch: 70\ttrain_loss=1.33414, train_acc=0.19910 val_roc=0.80064 val_ap=0.75729\n",
      "Epoch: 71\ttrain_loss=1.29482, train_acc=0.18461 val_roc=0.82176 val_ap=0.79225\n",
      "Epoch: 72\ttrain_loss=1.20038, train_acc=0.18236 val_roc=0.83293 val_ap=0.79242\n",
      "Epoch: 73\ttrain_loss=1.37579, train_acc=0.12704 val_roc=0.81335 val_ap=0.75637\n",
      "Epoch: 74\ttrain_loss=1.04521, train_acc=0.21542 val_roc=0.82306 val_ap=0.80292\n",
      "Epoch: 75\ttrain_loss=1.23740, train_acc=0.15648 val_roc=0.83701 val_ap=0.81634\n",
      "Epoch: 76\ttrain_loss=1.01996, train_acc=0.20789 val_roc=0.80467 val_ap=0.76914\n",
      "Epoch: 77\ttrain_loss=1.17511, train_acc=0.15917 val_roc=0.74614 val_ap=0.68890\n",
      "Epoch: 78\ttrain_loss=0.96210, train_acc=0.16652 val_roc=0.83481 val_ap=0.80931\n",
      "Epoch: 79\ttrain_loss=0.95085, train_acc=0.17398 val_roc=0.83184 val_ap=0.81853\n",
      "Epoch: 80\ttrain_loss=0.94510, train_acc=0.16952 val_roc=0.83609 val_ap=0.80390\n",
      "Epoch: 81\ttrain_loss=0.98430, train_acc=0.14604 val_roc=0.81612 val_ap=0.77399\n",
      "Epoch: 82\ttrain_loss=0.90979, train_acc=0.19755 val_roc=0.82969 val_ap=0.81834\n",
      "Epoch: 83\ttrain_loss=0.95164, train_acc=0.17585 val_roc=0.82831 val_ap=0.81632\n",
      "Epoch: 84\ttrain_loss=0.93087, train_acc=0.26579 val_roc=0.77662 val_ap=0.74716\n",
      "Epoch: 85\ttrain_loss=1.04090, train_acc=0.14684 val_roc=0.78165 val_ap=0.72625\n",
      "Epoch: 86\ttrain_loss=0.89080, train_acc=0.22260 val_roc=0.81891 val_ap=0.81050\n",
      "Epoch: 87\ttrain_loss=0.91300, train_acc=0.20103 val_roc=0.82903 val_ap=0.82162\n",
      "Epoch: 88\ttrain_loss=0.89476, train_acc=0.25869 val_roc=0.81331 val_ap=0.78802\n",
      "Epoch: 89\ttrain_loss=1.00720, train_acc=0.18266 val_roc=0.77253 val_ap=0.72417\n",
      "Epoch: 90\ttrain_loss=0.84736, train_acc=0.25710 val_roc=0.82442 val_ap=0.80633\n",
      "Epoch: 91\ttrain_loss=0.92579, train_acc=0.17426 val_roc=0.84565 val_ap=0.83832\n",
      "Epoch: 92\ttrain_loss=0.85513, train_acc=0.31394 val_roc=0.79519 val_ap=0.77720\n",
      "Epoch: 93\ttrain_loss=0.91000, train_acc=0.23849 val_roc=0.78309 val_ap=0.74682\n",
      "Epoch: 94\ttrain_loss=0.85805, train_acc=0.24336 val_roc=0.83620 val_ap=0.81739\n",
      "Epoch: 95\ttrain_loss=0.84066, train_acc=0.26238 val_roc=0.82655 val_ap=0.81832\n",
      "Epoch: 96\ttrain_loss=0.82948, train_acc=0.31158 val_roc=0.80898 val_ap=0.79961\n",
      "Epoch: 97\ttrain_loss=0.85909, train_acc=0.27955 val_roc=0.82617 val_ap=0.80180\n",
      "Epoch: 98\ttrain_loss=0.87218, train_acc=0.25963 val_roc=0.83676 val_ap=0.80691\n",
      "Epoch: 99\ttrain_loss=1.00582, train_acc=0.31068 val_roc=0.82485 val_ap=0.80139\n",
      "Epoch: 100\ttrain_loss=0.83838, train_acc=0.27873 val_roc=0.83753 val_ap=0.82502\n",
      "End of training!\n",
      "test_roc=0.83641\n",
      "test_ap=0.82393\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Training and validation using a full graph\n",
    "    vgae_model.train()\n",
    "    logits = vgae_model.forward(graph, features)\n",
    "\n",
    "    # compute loss\n",
    "    loss = norm * F.binary_cross_entropy(\n",
    "        logits.view(-1), adj_label.to_dense().view(-1), weight=weight_tensor\n",
    "    )\n",
    "    kl_divergence = (\n",
    "        0.5\n",
    "        / logits.size(0)\n",
    "        * (\n",
    "            1\n",
    "            + 2 * vgae_model.log_std\n",
    "            - vgae_model.mean**2\n",
    "            - torch.exp(vgae_model.log_std) ** 2\n",
    "        )\n",
    "        .sum(1)\n",
    "        .mean()\n",
    "    )\n",
    "    loss -= kl_divergence\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = get_acc(logits, adj_label)\n",
    "    val_roc, val_ap = get_scores(val_edges, val_edges_false, logits)\n",
    "    print(f'Epoch: {epoch+1}\\ttrain_loss={loss.item():.5f}, train_acc={train_acc:.5f} val_roc={val_roc:.5f} val_ap={val_ap:.5f}')\n",
    "\n",
    "test_roc, test_ap = get_scores(test_edges, test_edges_false, logits)\n",
    "print(\n",
    "    f'End of training!\\ntest_roc={test_roc:.5f}\\ntest_ap={test_ap:.5f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final area under the ROC curve (AUC): 0.8375330884095715\n",
      "Final avarage precision (AP): 0.8250215729704059\n"
     ]
    }
   ],
   "source": [
    "print(\"Final area under the ROC curve (AUC):\" , val_roc)\n",
    "print(\"Final avarage precision (AP):\", val_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subject area the most common evaluations are the AUC (area under the ROC curve) and AP (avarage precision).\n",
    "The AP on the first epoch was close to 50 %, meaning it was almost random, but for the 10th epoch it became 60% (which isn`t too much, but it will be better).\n",
    "\n",
    "For more evaluation we choose the AUC metric. The reason behind it is, that the most important part of the project is to find all the interactions between drugs correctly if possible. So this metric checks that how precise is an interaction predicted by us. Meaning, if we predicted that between two drugs there is an interaction, what is the chance that there really is one. We chose this because it focuses on the finding more edges correctly, rather than finding non-edges correctly. \n",
    "As seen above this metric also went above 50% after a few epochs so it is working correctly. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "006bcdd8519914a6a61e79853e7c25c2d14f28d303be2d51e2ef8bb5a650da56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
