{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ec40a12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in ./venv/lib/python3.10/site-packages (4.5.3)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.10/site-packages (from gdown) (4.64.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from gdown) (3.8.0)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.10/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: requests[socks] in ./venv/lib/python3.10/site-packages (from gdown) (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.10/site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2022.9.24)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in ./venv/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Retrieving folder list\n",
      "Processing file 1Ot-ICpiJRlisFvM9Fi6TM3Q6kAZaLS0y ChCh-Miner_durgbank-chem-chem.tsv.gz\n",
      "Processing file 1LSdAthCa69kWRIKoI5UmclLgf4OsSNAm drugbank_all_full_database.xml.zip\n",
      "Processing file 15_hqow9NT_M49OX7cXrG5P6vCgfbKyhP drugbank.xsd\n",
      "Retrieving folder list completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Ot-ICpiJRlisFvM9Fi6TM3Q6kAZaLS0y\n",
      "To: /home/i/HDD/BME/7/deeplearning/nhf-drugs/drug-drug/ChCh-Miner_durgbank-chem-chem.tsv.gz\n",
      "100%|████████████████████████████████████████| 207k/207k [00:00<00:00, 5.62MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1LSdAthCa69kWRIKoI5UmclLgf4OsSNAm\n",
      "To: /home/i/HDD/BME/7/deeplearning/nhf-drugs/drug-drug/drugbank_all_full_database.xml.zip\n",
      "100%|████████████████████████████████████████| 152M/152M [00:05<00:00, 26.4MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=15_hqow9NT_M49OX7cXrG5P6vCgfbKyhP\n",
      "To: /home/i/HDD/BME/7/deeplearning/nhf-drugs/drug-drug/drugbank.xsd\n",
      "100%|██████████████████████████████████████| 43.2k/43.2k [00:00<00:00, 2.62MB/s]\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "# Get the DrugBank XML database\n",
    "!pip install gdown # downloading files from google drive.\n",
    "!gdown --folder https://drive.google.com/drive/folders/1hZa_Vc9dZf_oyNjQoCsO2eKVAOzz-78e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5f12618",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -p drug-drug/drugbank_all_full_database.xml.zip > drug-drug/full_database.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5177f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompress the graph structure descriptor TSV file\n",
    "!gzip -dk drug-drug/ChCh-Miner_durgbank-chem-chem.tsv.gz # unzip, keep original .gz file as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb7628c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug-drug/full_database.xml: XML 1.0 document, ASCII text, with very long lines (537)\r\n"
     ]
    }
   ],
   "source": [
    "!file drug-drug/full_database.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd4259e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DB00862</td>\n",
       "      <td>DB00966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DB00575</td>\n",
       "      <td>DB00806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DB01242</td>\n",
       "      <td>DB08893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DB01151</td>\n",
       "      <td>DB08883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DB01235</td>\n",
       "      <td>DB01275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48509</th>\n",
       "      <td>DB00542</td>\n",
       "      <td>DB01354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48510</th>\n",
       "      <td>DB00476</td>\n",
       "      <td>DB01239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48511</th>\n",
       "      <td>DB00621</td>\n",
       "      <td>DB01120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48512</th>\n",
       "      <td>DB00808</td>\n",
       "      <td>DB01356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48513</th>\n",
       "      <td>DB00677</td>\n",
       "      <td>DB06287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48514 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0        1\n",
       "0      DB00862  DB00966\n",
       "1      DB00575  DB00806\n",
       "2      DB01242  DB08893\n",
       "3      DB01151  DB08883\n",
       "4      DB01235  DB01275\n",
       "...        ...      ...\n",
       "48509  DB00542  DB01354\n",
       "48510  DB00476  DB01239\n",
       "48511  DB00621  DB01120\n",
       "48512  DB00808  DB01356\n",
       "48513  DB00677  DB06287\n",
       "\n",
       "[48514 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "edges_df = pd.read_csv('drug-drug/ChCh-Miner_durgbank-chem-chem.tsv', sep='\\t', header=None, index_col=False)\n",
    "edges_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "946908de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DB00005', 'DB00006', 'DB00007', ..., 'DB11256', 'DB11315',\n",
       "       'DB11354'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertices = np.unique(edges_df.values)\n",
    "vertices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffa9122",
   "metadata": {},
   "source": [
    "## Generating the training, validation and test data\n",
    "\n",
    "The article suggests that the auto-encoder builds a latent representation ($\\textbf{Z}$) based on the $\\textbf{X}$ feature vectors and the $\\textbf{A}$ adjacency matrix, then the decoder (the generative model) predicts $\\textbf{A}$ based on $\\textbf{Z}$, with pairs of vertices whose $\\textbf{z}$ latent representations have a higher inner product being more likely to be connected.\n",
    "\n",
    "The goal of the prediction is to determine the existence of an edge between any two nodes. Any edge should be in at most one of the *train*, *validation* or *split* sets, otherwise we would have, for example, validation data that was part of the training data as well, defeating the purpose and integrity of the validation set. As a consequence, the best way to split the data is to partition the edges into *train edges*, *validation edges* and *test edges*.\n",
    "\n",
    "The model proposed uses graph convolutional layers to extract the latent variables ($\\textbf{Z}$), and graph convolutions use vertex-local structure, with information from vertices closer than $d$ hops, where $d$ is the convolutional filter's size. When selecting samples, these samples should be subgraphs (as opposed to one, or few pairs of nodes per sample, querying wether they are connected or not), to keep some of the valid information about connected-ness intact.\n",
    "\n",
    "The three-way split defines three separate subgraphs, and the samples will be subgraphs of these. During the split, it is possible that, for example, for some vertex $a$, edge $(a, x)$ is in the *train* split, $(a, y)$ is in *validation* and $(a, z)$ is in *test*, so during training, the majority of the information about the vertex's neighbourhood is lost. We solve this problem by keeping the percentage of *training* edges high, to render this situation unlikely.\n",
    "\n",
    "Furthermore, it is also possible that during the sampling of the *training* graph, *training* edges connected to the same vertex get split up across samples, so we reduce the information that the model 'sees', at that particular sample, with respect to that given vertex. This is not necessarily a problem, because this situation can condition the model to be more redundant (in the way a *dropout* layer can condition a model using fully connected layers to be more redundant). However, we will solve this problem, because so far, the feature extraction from the database does not give much to rely on, so neighbourhood and clustering information between interacting molecules can be critical. We will sample a fixed number of vertices from the *training* subgraph first, then we will include all the edges between these vertices that are in the *training* graph - also called an *induced subgraph* (same for the other splits).\n",
    "\n",
    "It is crucial, that when *loss* is calculated, we should **not penalize** the model for finding an edge that is **not in the *training* sample**, but **is present in the original graph** (the same applies to the *validation* and *test* steps as well). Supppose we have a training sample $(X, y)$, where $X=(G, features)$, $y=G'$, and the model should infer $y$ from $X$. $G$ should be a subgraph of the *training graph* (to keep the integrity of the *train*, *validation* and *test* sets - see the second paragraph of this section), but $G'$ should be the maximal subgraph *of the original graph* that has exactly the same vertices as $G$ (for correctness).\n",
    "\n",
    "Applying the principles above, representing the $G$ and $G'$ graphs as a list of edges, we generate the *train*, *validation* and *test* sets as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b15066fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training subgraph:\n",
      "\tEdges:29108\n",
      "\tVertices:1448\n",
      "\n",
      "Validation subgraph:\n",
      "\tEdges:9703\n",
      "\tVertices:1335\n",
      "\n",
      "Training subgraph:\n",
      "\tEdges:9703\n",
      "\tVertices:1317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input representation for the whole graph\n",
    "N = vertices.shape[0] # we have about 1500 vertices\n",
    "M = edges_df.shape[0] # we have about 48000 edges\n",
    "F = 10 # insert number of relevant features\n",
    "features = np.random.uniform(size=(N, F)) # insert values parsed from XML\n",
    "\n",
    "# Train-val-test split\n",
    "test_percentage = 0.2\n",
    "val_percentage = 0.2\n",
    "val_split = int((1.0 - test_percentage) * M)\n",
    "train_split = int((1.0 - val_percentage - test_percentage) * M)\n",
    "\n",
    "# Partitioning edges randomly into either category\n",
    "all_edges = edges_df.values\n",
    "shuffled_edges = edges_df.sample(frac=1.0).reset_index(drop=True)\n",
    "train_edges = shuffled_edges.iloc[:train_split].values\n",
    "train_vertices = np.unique(train_edges)\n",
    "val_edges = shuffled_edges.iloc[train_split:val_split].values\n",
    "val_vertices = np.unique(val_edges)\n",
    "test_edges = shuffled_edges.iloc[val_split:].values\n",
    "test_vertices = np.unique(test_edges)\n",
    "# Print\n",
    "print(f'Training subgraph:\\n\\tEdges:{train_edges.shape[0]}\\n\\tVertices:{train_vertices.shape[0]}\\n')\n",
    "print(f'Validation subgraph:\\n\\tEdges:{val_edges.shape[0]}\\n\\tVertices:{val_vertices.shape[0]}\\n')\n",
    "print(f'Training subgraph:\\n\\tEdges:{test_edges.shape[0]}\\n\\tVertices:{test_vertices.shape[0]}\\n')\n",
    "# train_edges, val_edges, test_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c6bca46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 60/60 [00:07<00:00,  8.24it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 20/20 [00:01<00:00, 11.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 20/20 [00:01<00:00, 11.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# the following numbers are only an example, the real data should be at least two magnitudes bigger\n",
    "nb_vertices = 50 # number of vertices in a sample\n",
    "nb_train = 60 # number of train samples\n",
    "nb_val = 20 # number of validation samples\n",
    "nb_test = 20 # number of test samples\n",
    "\n",
    "# We gather the samples in the corresponding list\n",
    "train, val, test = [], [], []\n",
    "\n",
    "for _ in tqdm(range(nb_train)):\n",
    "    V = np.random.choice(train_vertices, size=nb_vertices, replace=False)\n",
    "    E = train_edges[\n",
    "        np.isin(train_edges[:, 0], V) & np.isin(train_edges[:, 1], V)\n",
    "    ] # edges from 'train' where both vertices are in V\n",
    "    E_eval = all_edges[\n",
    "        np.isin(all_edges[:, 0], V) & np.isin(all_edges[:, 1], V)\n",
    "    ] # any valid edge where both vertices are in V\n",
    "    train.append([V, E, E_eval])\n",
    "\n",
    "for _ in tqdm(range(nb_val)):\n",
    "    V = np.random.choice(val_vertices, size=nb_vertices, replace=False)\n",
    "    E = val_edges[\n",
    "        np.isin(val_edges[:, 0], V) & np.isin(val_edges[:, 1], V)\n",
    "    ] # edges from 'val' where both vertices are in V\n",
    "    E_eval = all_edges[\n",
    "        np.isin(all_edges[:, 0], V) & np.isin(all_edges[:, 1], V)\n",
    "    ] # any valid edge where both vertices are in V\n",
    "    val.append([V, E, E_eval])\n",
    "    \n",
    "for _ in tqdm(range(nb_test)):\n",
    "    V = np.random.choice(test_vertices, size=nb_vertices, replace=False)\n",
    "    E = test_edges[\n",
    "        np.isin(test_edges[:, 0], V) & np.isin(test_edges[:, 1], V)\n",
    "    ] # edges from 'test' where both vertices are in V\n",
    "    E_eval = all_edges[\n",
    "        np.isin(all_edges[:, 0], V) & np.isin(all_edges[:, 1], V)\n",
    "    ] # any valid edge where both vertices are in V\n",
    "    test.append([V, E, E_eval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f8eef259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train, val and test folders\n",
    "import os\n",
    "\n",
    "def mkdir(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "os.mkdir('data')\n",
    "\n",
    "train_path = os.path.join('data', 'train')\n",
    "os.mkdir(train_path)\n",
    "\n",
    "val_path = os.path.join('data', 'val')\n",
    "os.mkdir(val_path)\n",
    "\n",
    "test_path = os.path.join('data', 'test')\n",
    "os.mkdir(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c6fd6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [00:02, 29.17it/s] \n",
      "20it [00:00, 328.16it/s]\n",
      "20it [00:00, 332.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# write the data to files\n",
    "for i, (V, E, E_eval) in tqdm(enumerate(train)):\n",
    "    save_path = os.path.join('data', 'train', str(i))\n",
    "    os.mkdir(save_path)\n",
    "    np.savetxt(os.path.join(save_path, 'input_v'), V, fmt='%s')\n",
    "    np.savetxt(os.path.join(save_path, 'input_e'), E, fmt='%s')\n",
    "    np.savetxt(os.path.join(save_path, 'output_e'), E_eval, fmt='%s')\n",
    "    \n",
    "for i, (V, E, E_eval) in tqdm(enumerate(val)):\n",
    "    save_path = os.path.join('data', 'val', str(i))\n",
    "    os.mkdir(save_path)\n",
    "    np.savetxt(os.path.join(save_path, 'input_v'), V, fmt='%s')\n",
    "    np.savetxt(os.path.join(save_path, 'input_e'), E, fmt='%s')\n",
    "    np.savetxt(os.path.join(save_path, 'output_e'), E_eval, fmt='%s')\n",
    "    \n",
    "for i, (V, E, E_eval) in tqdm(enumerate(test)):\n",
    "    save_path = os.path.join('data', 'test', str(i))\n",
    "    os.mkdir(save_path)\n",
    "    np.savetxt(os.path.join(save_path, 'input_v'), V, fmt='%s')\n",
    "    np.savetxt(os.path.join(save_path, 'input_e'), E, fmt='%s')\n",
    "    np.savetxt(os.path.join(save_path, 'output_e'), E_eval, fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
